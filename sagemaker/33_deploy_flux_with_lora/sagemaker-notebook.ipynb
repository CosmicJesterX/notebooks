{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5a812b",
   "metadata": {},
   "source": [
    "# FLUX.1 with LoRA on Amazon SageMaker \n",
    "\n",
    "Welcome to this Amazon SageMaker guide on how to deploy the [FLUX.1 model](black-forest-labs/FLUX.1-dev) with multi LoRA Adapters to Amazon SageMaker. We will deploy the FLUX.1 model with multiple adeptable LoRA Adapters to Amazon SageMake for real-time inference using Hugging Faces [ðŸ§¨ Diffusers library](https://huggingface.co/docs/diffusers/index).\n",
    "\n",
    "![stable-diffusion-on-amazon-sagemaker](./imgs/sd-on-sm.png)\n",
    "\n",
    "What we are going to do \n",
    "1. Create FLUX.1 inference script \n",
    "2. Deploy FLUX.1 with LoRA Adapters to Amazon SageMaker\n",
    "3. Generate images using FLUX.1 schnell\n",
    "\n",
    "\n",
    "## What is FLUX.1?\n",
    "\n",
    "FLUX is an open-weights 12B parameter family of rectified flow transformers that generates images from text descriptions, pushing the boundaries of text-to-image generation created by Black Forest Labs. It comes in three variants, each catering to different use cases:\n",
    "\n",
    "* FLUX.1 [pro]: Offers top-tier performance for commercial applications with high visual quality and prompt adherence, available via API and enterprise solutions.\n",
    "* FLUX.1 [dev]: Open-weight model for non-commercial use, providing similar capabilities to FLUX.1 [pro], available on platforms like HuggingFace.\n",
    "* FLUX.1 [schnell]: Fastest model, designed for local development and personal use, available under the Apache 2.0 license with open inference code on GitHub.\n",
    "\n",
    "\n",
    "![schnell_grid](./imgs/schnell_grid.jpeg)\n",
    "\n",
    "--- \n",
    "\n",
    "Before we can get started, make sure you have [Hugging Face user account](https://huggingface.co/join). The account is needed to load the [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) from the [Hugging Face Hub](https://huggingface.co/).\n",
    "\n",
    "Create account: https://huggingface.co/join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.231.0\" \"huggingface_hub==0.24.6\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a206a22",
   "metadata": {},
   "source": [
    "## Create Multi-LoRA FLUX.1 inference script \n",
    "\n",
    "Amazon SageMaker allows us to customize the inference script by providing a `inference.py` file. The `inference.py` file is the entry point to our model. It is responsible for loading the model and handling the inference request. If you are used to deploying Hugging Face Transformers that might be knew to you. Usually, we just provide the `HF_MODEL_ID` and `HF_TASK` and the Hugging Face DLC takes care of the rest.\n",
    "\n",
    "For multi-LoRA FLUX.1 we need to provide the LoRA Adapters and the base FLUX.1 model. We will use the `diffusers` library to load the model and handle the inference request. We create a custom `model_fn` and `predict_fn` to load the model and handle the inference request.\n",
    "\n",
    "\n",
    "If you want to learn more about creating a custom inference script you can check out [Creating document embeddings with Hugging Face's Transformers & Amazon SageMaker](https://www.philschmid.de/custom-inference-huggingface-sagemaker)\n",
    "\n",
    "In addition to the `inference.py` file we also have to provide a `requirements.txt` file. The `requirements.txt` file is used to install the dependencies for our `inference.py` file.\n",
    "\n",
    "The first step is to create a `code/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a574f",
   "metadata": {},
   "source": [
    "As next we create a `requirements.txt` file and add the latest `diffusers` library to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf302de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "diffusers==0.30.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c2ca9",
   "metadata": {},
   "source": [
    "Next we need to create the `inference.py` file. The `inference.py` file is responsible for loading the model and handling the inference request. The `model_fn` function is called when the model is loaded. The `predict_fn` function is called when we want to do inference. \n",
    "\n",
    "We need to update the `model_fn` that is uses the `HF_MODEL_ID` for FLUX.1-dev and `HF_ADAPTER_IDS` for each LoRA Adapter. \n",
    "\n",
    "_Note: The `HF_ADAPTER_IDS` is a list of LoRA Adapter IDs. The LoRA Adapter IDs are the names of the LoRA Adapters. The LoRA Adapter IDs are used to load the LoRA Adapters from the Hugging Face Hub. This environment variable is not included when you deploy the model without a custom inference script._\n",
    "\n",
    "In the `predict_fn` we first validate which adapter id is requrested and then generate 4 image for an input prompt. The `predict_fn` function returns the `4` image as a `base64` encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fe297e76104f72857c8fc715d82d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac26c89676a49c580a653baa8ac4024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%writefile code/inference.py\n",
    "import base64\n",
    "import torch\n",
    "import os \n",
    "import json\n",
    "from io import BytesIO\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "# HF_ADAPTER_IDS=\"{\\\"ostris/yearbook-photo-flux-schnell\\\": \\\"yearbook-photo-flux-schnell-v1.safetensors\\\"}\" python inference.py\n",
    "\n",
    "# ADAPTER_IDS needs to be a JSON object with the adapter id as key and the adapter weight name as value\n",
    "# e.g. {\"ostris/yearbook-photo-flux-schnell\": \"yearbook-photo-flux-schnell-v1.safetensors\"}\n",
    "ADAPTERS = json.loads(os.getenv(\"HF_ADAPTER_IDS\", \"{}\"))\n",
    "\n",
    "MODEL_ID = os.getenv(\"HF_MODEL_ID\", \"black-forest-labs/FLUX.1-schnell\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model from Hugging Face and apply the LoRA weights if provided.\"\"\"\n",
    "    pipeline = AutoPipelineForText2Image.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16, device_map=\"balanced\")\n",
    "    if len(ADAPTERS.keys()) > 0:\n",
    "        for adapter_id in ADAPTERS.keys():\n",
    "            print(f\"Loading adapter: {adapter_id}\")\n",
    "            pipeline.load_lora_weights(adapter_id, weight_name=ADAPTERS[adapter_id], adapter_name=adapter_id)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "    \"\"\"Run the model with the provided data and return the generated images.\"\"\"\n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    "    \n",
    "    # check if adapter id is provided\n",
    "    adapter_id = data.pop(\"adapter_id\", None)\n",
    "    # if adapter id is provided, set the adapter\n",
    "    if ADAPTERS.get(adapter_id, None) is not None:\n",
    "        print(f\"Using adapter: {adapter_id}\")\n",
    "        pipe.set_adapters(adapter_id)\n",
    "    else:\n",
    "        print(f\"No valid adapter id provided, using base model\")\n",
    "        pipe.disable_lora()\n",
    "    \n",
    "    # set valid HP for stable diffusion\n",
    "    num_inference_steps = data.pop(\"num_inference_steps\", 4) # only need 4 for schnell version, dev version needs 20-30 or so               \n",
    "    guidance_scale = data.pop(\"guidance_scale\", 0)  # must be 0.0 for schnell version, dev version can be 3.5    \n",
    "    num_images_per_prompt = data.pop(\"num_images_per_prompt\", 4)\n",
    "\n",
    "    # run generation with parameters\n",
    "    generated_images = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        max_sequence_length=256,\n",
    "        generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "    )[\"images\"]\n",
    "\n",
    "    # create response\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # create response\n",
    "    return {\"generated_images\": encoded_images}\n",
    "\n",
    "\n",
    "model = model_fn(\".\")\n",
    "# payload = {\"inputs\": \"Headshot of handsome young man, wearing dark gray sweater with buttons and big shawl collar, brown hair and short beard, soft studio lighting, portrait photography --ar 85:128 --v 6.0 --style raw\", \"adapter_id\": \"ostris/yearbook-photo-flux-schnell\"}\n",
    "# result = predict_fn(payload, model)\n",
    "\n",
    "# for i, image in enumerate(result[\"generated_images\"]):\n",
    "#     # save image to file\n",
    "#     with open(f\"image_{i}.jpg\", \"wb\") as f:\n",
    "#         f.write(base64.b64decode(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ff49f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer': 'cpu', 'text_encoder_2': 3, 'text_encoder': 2, 'vae': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "## Deploy FLUX.1 with LoRA Adapters to Amazon SageMaker\n",
    "\n",
    "In this example we will deploy [black-forest-labs/FLUX.1-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell). FlUX.1-schnell is a fast version of FLUX.1 available under the Apache 2.0. It only needs 4 inference steps to generate a high quality image. This is great for real-time inference and local development. If you plan to use FLUX.1 dev you need to switch to an asynchronous inference endpoint or need a bigger instance type (A100/H100). Why? Amazon SageMaker has a request timeout of 60s. Depending on the instance type you use the generation might take longer. In our example we will use an `ml.g5.2xlarge` instance with 1 NVIDIA A10G GPUs of 24GB memory. This is not enough memory to load FLUX.1 and parts of it will be loaded on the CPU. Offloading to CPU will slow down the generation. \n",
    "\n",
    "We will also use 2 LoRA Adapters, [prithivMLmods/Canopus-LoRA-Flux-FaceRealism](https://huggingface.co/prithivMLmods/Canopus-LoRA-Flux-FaceRealism) and [ostris/yearbook-photo-flux-schnell](htthttps://huggingface.co/ostris/yearbook-photo-flux-schnell).\n",
    "\n",
    "You can find more LoRA Adapters for FLUX.1 schnell [here](https://huggingface.co/models?other=base_model:adapter:black-forest-labs/FLUX.1-schnell) or you can train you own using the [AI Toolkit by Ostris](https://github.com/ostris/ai-toolkit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4534bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import torch\n",
    "import os \n",
    "os.environ[\"HF_ADAPTER_IDS\"] = \"{\\\"ostris/yearbook-photo-flux-schnell\\\": \\\"yearbook-photo-flux-schnell-v1.safetensors\\\"}\"\n",
    "import json\n",
    "from io import BytesIO\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "# HF_ADAPTER_IDS=\"{\\\"ostris/yearbook-photo-flux-schnell\\\": \\\"yearbook-photo-flux-schnell-v1.safetensors\\\"}\" python inference.py\n",
    "\n",
    "# ADAPTER_IDS needs to be a JSON object with the adapter id as key and the adapter weight name as value\n",
    "# e.g. {\"ostris/yearbook-photo-flux-schnell\": \"yearbook-photo-flux-schnell-v1.safetensors\"}\n",
    "ADAPTERS = json.loads(os.getenv(\"HF_ADAPTER_IDS\", \"{}\"))\n",
    "\n",
    "MODEL_ID = os.getenv(\"HF_MODEL_ID\", \"black-forest-labs/FLUX.1-schnell\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model from Hugging Face and apply the LoRA weights if provided.\"\"\"\n",
    "    pipeline = AutoPipelineForText2Image.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16, device_map=\"balanced\")\n",
    "    if len(ADAPTERS.keys()) > 0:\n",
    "        for adapter_id in ADAPTERS.keys():\n",
    "            print(f\"Loading adapter: {adapter_id}\")\n",
    "            pipeline.load_lora_weights(adapter_id, weight_name=ADAPTERS[adapter_id], adapter_name=adapter_id)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "    \"\"\"Run the model with the provided data and return the generated images.\"\"\"\n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    "    \n",
    "    # check if adapter id is provided\n",
    "    adapter_id = data.pop(\"adapter_id\", None)\n",
    "    # if adapter id is provided, set the adapter\n",
    "    if ADAPTERS.get(adapter_id, None) is not None:\n",
    "        print(f\"Using adapter: {adapter_id}\")\n",
    "        pipe.set_adapters(adapter_id)\n",
    "    else:\n",
    "        print(f\"No valid adapter id provided, using base model\")\n",
    "        pipe.disable_lora()\n",
    "    \n",
    "    # set valid HP for stable diffusion\n",
    "    num_inference_steps = data.pop(\"num_inference_steps\", 4) # only need 4 for schnell version, dev version needs 20-30 or so               \n",
    "    guidance_scale = data.pop(\"guidance_scale\", 0)  # must be 0.0 for schnell version, dev version can be 3.5    \n",
    "    num_images_per_prompt = data.pop(\"num_images_per_prompt\", 4)\n",
    "\n",
    "    # run generation with parameters\n",
    "    generated_images = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        max_sequence_length=256,\n",
    "        generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "    )[\"images\"]\n",
    "\n",
    "    # create response\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # create response\n",
    "    return {\"generated_images\": encoded_images}\n",
    "\n",
    "\n",
    "model = model_fn(\".\")\n",
    "payload = {\"inputs\": \"Headshot of handsome young man, wearing dark gray sweater with buttons and big shawl collar, brown hair and short beard, soft studio lighting, portrait photography --ar 85:128 --v 6.0 --style raw\", \"adapter_id\": \"ostris/yearbook-photo-flux-schnell\"}\n",
    "result = predict_fn(payload, model)\n",
    "\n",
    "for i, image in enumerate(result[\"generated_images\"]):\n",
    "    # save image to file\n",
    "    with open(f\"image_{i}.jpg\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",       # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3812f",
   "metadata": {},
   "source": [
    "## Generate images using the deployed model\n",
    "\n",
    "The `.deploy()` returns an `HuggingFacePredictor` object which can be used to request inference. Our endpoint expects a `json` with at least `inputs` key. The `inputs` key is the input prompt for the model, which will be used to generate the image. Additionally, we can provide `num_inference_steps`, `guidance_scale` & `num_images_per_prompt` to controll the generation.\n",
    "\n",
    "The `predictor.predict()` function returns a `json` with the `generated_images` key. The `generated_images` key contains the `4` generated images as a `base64` encoded string. To decode our response we added a small helper function `decode_base64_to_image` which takes the `base64` encoded string and returns a `PIL.Image` object and `display_images`, which takes a list of `PIL.Image` objects and displays them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "  base64_image = base64.b64decode(image_string)\n",
    "  buffer = BytesIO(base64_image)\n",
    "  return Image.open(buffer)\n",
    "\n",
    "# display PIL images as grid\n",
    "def display_images(images=None,columns=3, width=100, height=100):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71647be",
   "metadata": {},
   "source": [
    "Now, lets generate some images. As example lets generate `3` images for the prompt `A dog trying catch a flying pizza art drawn by disney concept artists`. Generating `3` images takes around `30` seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d253d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_prompt = 3\n",
    "prompt = \"A dog trying catch a flying pizza art drawn by disney concept artists, golden colour, high quality, highly detailed, elegant, sharp focus\"\n",
    "\n",
    "# run prediction\n",
    "response = predictor.predict(data={\n",
    "  \"inputs\": prompt,\n",
    "  \"num_images_per_prompt\" : num_images_per_prompt\n",
    "  }\n",
    ")\n",
    "\n",
    "# decode images\n",
    "decoded_images = [decode_base64_image(image) for image in response[\"generated_images\"]]\n",
    "\n",
    "# visualize generation\n",
    "display_images(decoded_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10007d",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6fb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
